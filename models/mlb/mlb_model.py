# models/mlb/mlb_model.py

import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Any, Optional, Tuple
import joblib
from pathlib import Path
from datetime import datetime
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

from config.settings import Settings

class MLBPredictionModel:
    """
    Comprehensive MLB prediction model using XGBoost.
    Supports game outcome, total runs, and player prop predictions.
    """
    
    def __init__(self, model_type: str = 'game_winner'):
        """
        Initialize MLB prediction model.
        
        Args:
            model_type: Type of model ('game_winner', 'total_runs', 'nrfi', 'player_hits', etc.)
        """
        self.model_type = model_type
        self.model = None
        self.scaler = None
        self.feature_names = []
        self.feature_importance = {}
        
        # Get model parameters from settings
        self.model_params = Settings.get_model_params('mlb')
        
        # Adjust parameters based on model type
        if model_type == 'game_winner':
            self.model_params['objective'] = 'binary:logistic'
            self.model_params['eval_metric'] = 'logloss'
            self.target_column = 'home_win'
        elif model_type == 'total_runs':
            self.model_params['objective'] = 'reg:squarederror'
            self.model_params['eval_metric'] = 'rmse'
            self.target_column = 'total_runs'
        elif model_type == 'nrfi':  # No Run First Inning
            self.model_params['objective'] = 'binary:logistic'
            self.model_params['eval_metric'] = 'logloss'
            self.target_column = 'nrfi'  # 1 if no runs in 1st inning, 0 if runs scored
        elif 'player' in model_type:
            self.model_params['objective'] = 'reg:squarederror'
            self.model_params['eval_metric'] = 'rmse'
            self.target_column = self._get_player_target_column(model_type)
        
        # Model storage path
        self.model_path = Settings.MODEL_PATHS['mlb'].get(model_type)
        
        logger.info(f"âš¾ Initialized MLB {model_type} model")
    
    def _get_player_target_column(self, model_type: str) -> str:
        """Get target column for player prop models."""
        player_targets = {
            'player_hits': 'hits',
            'player_rbi': 'rbi',
            'player_runs': 'runs',
            'player_strikeouts': 'strikeouts_pitched'
        }
        return player_targets.get(model_type, 'hits')
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Prepare features and target for training/prediction.
        
        Args:
            df: Input DataFrame with engineered features
            
        Returns:
            Tuple of (features_df, target_series)
        """
        logger.info(f"ðŸ”§ Preparing features for MLB {self.model_type} model...")
        
        # Remove non-feature columns
        exclude_columns = [
            'game_id', 'date', 'Date', 'season', 'inning', 'inning_half',
            'home_team_id', 'away_team_id', 'home_team_name', 'away_team_name', 
            'home_score', 'away_score', 'status', 'venue', 'city', 'weather',
            'created_at', 'updated_at', 'player_id', 'player_name', 'team_id'
        ]
        
        # Also exclude target columns we're not predicting
        target_columns = ['home_win', 'total_runs', 'nrfi', 'hits', 'rbi', 'runs', 'strikeouts_pitched']
        exclude_columns.extend([col for col in target_columns if col != self.target_column])
        
        # Select feature columns
        feature_columns = [col for col in df.columns 
                          if col not in exclude_columns and df[col].dtype in ['int64', 'float64']]
        
        # Prepare features
        X = df[feature_columns].copy()
        
        # Handle missing values in features
        X = X.fillna(X.median())
        
        # Prepare target
        if self.target_column in df.columns:
            y = df[self.target_column].copy()
            
            # Handle missing values in target
            valid_idx = y.notna()
            X = X[valid_idx]
            y = y[valid_idx]
        else:
            logger.warning(f"Target column '{self.target_column}' not found. Using dummy target.")
            y = pd.Series([0] * len(X))
        
        # Store feature names
        self.feature_names = list(X.columns)
        
        logger.info(f"âœ… Prepared {len(X)} samples with {len(self.feature_names)} features")
        logger.info(f"   Target: {self.target_column} (mean: {y.mean():.3f})")
        
        return X, y
    
    def train(self, df: pd.DataFrame, 
             validation_split: float = 0.2,
             cv_folds: int = 5,
             optimize_hyperparams: bool = False) -> Dict[str, Any]:
        """
        Train the MLB prediction model.
        
        Args:
            df: Training DataFrame with features and target
            validation_split: Fraction of data to use for validation
            cv_folds: Number of cross-validation folds
            optimize_hyperparams: Whether to perform hyperparameter optimization
            
        Returns:
            Dictionary with training results
        """
        logger.info(f"ðŸš€ Training MLB {self.model_type} model...")
        
        # Prepare features and target
        X, y = self.prepare_features(df)
        
        if len(X) == 0:
            raise ValueError("No valid training data after feature preparation")
        
        # Time series split for proper validation (respects temporal order)
        tscv = TimeSeriesSplit(n_splits=cv_folds)
        
        # Hyperparameter optimization if requested
        if optimize_hyperparams:
            logger.info("ðŸ” Optimizing hyperparameters...")
            best_params = self._optimize_hyperparameters(X, y, tscv)
            self.model_params.update(best_params)
        
        # Initialize model
        if self.model_type in ['game_winner', 'nrfi']:
            self.model = xgb.XGBClassifier(**self.model_params)
        else:
            self.model = xgb.XGBRegressor(**self.model_params)
        
        # Scale features
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # Train model with early stopping
        split_idx = int(len(X) * (1 - validation_split))
        X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
        y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
        
        # Train with early stopping
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
        
        # Cross-validation scores
        cv_scores = cross_val_score(
            self.model, X_scaled, y, 
            cv=tscv, 
            scoring='accuracy' if self.model_type in ['game_winner', 'nrfi'] else 'neg_mean_squared_error',
            n_jobs=-1
        )
        
        # Calculate feature importance
        self.feature_importance = dict(zip(
            self.feature_names, 
            self.model.feature_importances_
        ))
        
        # Validation predictions and metrics
        if self.model_type in ['game_winner', 'nrfi']:
            val_pred = self.model.predict(X_val)
            val_pred_proba = self.model.predict_proba(X_val)[:, 1]
            val_accuracy = accuracy_score(y_val, val_pred)
            val_auc = roc_auc_score(y_val, val_pred_proba)
            
            performance_metrics = {
                'validation_accuracy': val_accuracy,
                'validation_auc': val_auc,
                'cv_mean_accuracy': cv_scores.mean(),
                'cv_std_accuracy': cv_scores.std()
            }
        else:
            val_pred = self.model.predict(X_val)
            val_rmse = np.sqrt(np.mean((y_val - val_pred) ** 2))
            val_mae = np.mean(np.abs(y_val - val_pred))
            
            performance_metrics = {
                'validation_rmse': val_rmse,
                'validation_mae': val_mae,
                'cv_mean_rmse': np.sqrt(-cv_scores.mean()),
                'cv_std_rmse': np.sqrt(cv_scores.std())
            }
        
        training_results = {
            'model_type': self.model_type,
            'training_samples': len(X),
            'features_count': len(self.feature_names),
            'cv_folds': cv_folds,
            **performance_metrics,
            'top_features': self._get_top_features(10),
            'training_date': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… Training complete for MLB {self.model_type}")
        for metric, value in performance_metrics.items():
            logger.info(f"   {metric}: {value:.4f}")
        
        return training_results
    
    def predict(self, df: pd.DataFrame) -> np.ndarray:
        """Make predictions on new data."""
        if self.model is None:
            raise ValueError("Model not trained. Call train() first or load a trained model.")
        
        X, _ = self.prepare_features(df)
        
        if len(X) == 0:
            return np.array([])
        
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        
        logger.info(f"âœ… Generated {len(predictions)} predictions for MLB {self.model_type}")
        
        return predictions
    
    def predict_proba(self, df: pd.DataFrame) -> np.ndarray:
        """Get prediction probabilities (for classification models)."""
        if self.model_type not in ['game_winner', 'nrfi']:
            raise ValueError("predict_proba only available for classification models")
        
        if self.model is None:
            raise ValueError("Model not trained. Call train() first or load a trained model.")
        
        X, _ = self.prepare_features(df)
        
        if len(X) == 0:
            return np.array([])
        
        X_scaled = self.scaler.transform(X)
        probabilities = self.model.predict_proba(X_scaled)
        
        return probabilities
    
    def _optimize_hyperparameters(self, X: pd.DataFrame, y: pd.Series, cv) -> Dict[str, Any]:
        """Optimize hyperparameters using grid search."""
        param_grid = {
            'n_estimators': [150, 200, 250],
            'max_depth': [5, 7, 9],
            'learning_rate': [0.1, 0.12, 0.15],
            'subsample': [0.8, 0.9],
            'colsample_bytree': [0.8, 0.9]
        }
        
        if self.model_type in ['game_winner', 'nrfi']:
            base_model = xgb.XGBClassifier(**self.model_params)
            scoring = 'accuracy'
        else:
            base_model = xgb.XGBRegressor(**self.model_params)
            scoring = 'neg_mean_squared_error'
        
        grid_search = GridSearchCV(
            base_model, param_grid, 
            cv=cv, scoring=scoring, 
            n_jobs=-1, verbose=1
        )
        
        grid_search.fit(X, y)
        
        logger.info(f"ðŸŽ¯ Best hyperparameters: {grid_search.best_params_}")
        logger.info(f"ðŸŽ¯ Best CV score: {grid_search.best_score_:.4f}")
        
        return grid_search.best_params_
    
    def _get_top_features(self, n: int = 10) -> List[Tuple[str, float]]:
        """Get top N most important features."""
        if not self.feature_importance:
            return []
        
        sorted_features = sorted(
            self.feature_importance.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return sorted_features[:n]
    
    def save_model(self, filepath: Optional[Path] = None) -> Path:
        """Save the trained model to disk."""
        if self.model is None:
            raise ValueError("No model to save. Train a model first.")
        
        save_path = filepath or self.model_path
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        model_package = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'feature_importance': self.feature_importance,
            'model_type': self.model_type,
            'model_params': self.model_params,
            'target_column': self.target_column,
            'saved_date': datetime.now().isoformat()
        }
        
        joblib.dump(model_package, save_path)
        
        logger.info(f"ðŸ’¾ Saved MLB {self.model_type} model to {save_path}")
        return save_path
    
    def load_model(self, filepath: Optional[Path] = None) -> bool:
        """Load a trained model from disk."""
        load_path = filepath or self.model_path
        
        if not load_path.exists():
            logger.error(f"Model file not found: {load_path}")
            return False
        
        try:
            model_package = joblib.load(load_path)
            
            self.model = model_package['model']
            self.scaler = model_package['scaler']
            self.feature_names = model_package['feature_names']
            self.feature_importance = model_package.get('feature_importance', {})
            self.model_type = model_package.get('model_type', self.model_type)
            self.model_params = model_package.get('model_params', self.model_params)
            self.target_column = model_package.get('target_column', self.target_column)
            
            saved_date = model_package.get('saved_date', 'unknown')
            logger.info(f"ðŸ“¦ Loaded MLB {self.model_type} model from {load_path}")
            logger.info(f"   Model saved: {saved_date}")
            logger.info(f"   Features: {len(self.feature_names)}")
            
            return True
            
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            return False


class MLBModelEnsemble:
    """Ensemble of MLB prediction models for improved accuracy."""
    
    def __init__(self, model_types: List[str] = ['game_winner']):
        """Initialize ensemble with multiple model types."""
        self.models = {}
        self.weights = {}
        
        for model_type in model_types:
            self.models[model_type] = MLBPredictionModel(model_type)
            self.weights[model_type] = 1.0
        
        logger.info(f"ðŸŽ¯ Initialized MLB ensemble with {len(self.models)} models")
    
    def train_ensemble(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Train all models in the ensemble."""
        results = {}
        
        for model_type, model in self.models.items():
            logger.info(f"Training MLB {model_type} model...")
            try:
                model_results = model.train(df)
                results[model_type] = model_results
                
                # Adjust weights based on performance
                if model_type in ['game_winner', 'nrfi']:
                    performance = model_results.get('validation_accuracy', 0.5)
                else:
                    performance = 1.0 / (model_results.get('validation_rmse', 1.0) + 0.001)
                
                self.weights[model_type] = performance
                
            except Exception as e:
                logger.error(f"Error training MLB {model_type}: {e}")
                results[model_type] = {'error': str(e)}
        
        # Normalize weights
        total_weight = sum(self.weights.values())
        if total_weight > 0:
            self.weights = {k: v / total_weight for k, v in self.weights.items()}
        
        results['ensemble_weights'] = self.weights
        return results
    
    def predict_ensemble(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
        """Make ensemble predictions."""
        predictions = {}
        
        for model_type, model in self.models.items():
            try:
                if model.model is not None:
                    pred = model.predict(df)
                    predictions[model_type] = pred
            except Exception as e:
                logger.error(f"Error predicting with MLB {model_type}: {e}")
        
        return predictions
