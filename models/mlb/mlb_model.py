#!/usr/bin/env python3
"""
Updated MLB Model - Integrated with Pipeline
Combines enhanced pitcher-aware features with existing pipeline infrastructure
Uses API-Sports data (2021-2025) with 60-day testing split
Fixed odds processing for real live betting data
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
import json
import sys
from datetime import datetime, timedelta
from typing import Dict, Tuple, Any, Optional, List
import sqlite3
import warnings
warnings.filterwarnings('ignore')

# Add project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# ML Libraries
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif

# Advanced models
import xgboost as xgb
import lightgbm as lgb
from sklearn.neural_network import MLPClassifier

# Pipeline components with error handling
from loguru import logger

try:
    from config.settings import Settings
    SETTINGS_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è config.settings not available, using defaults")
    SETTINGS_AVAILABLE = False
    class Settings:
        MLB_DB_PATH = "data/mlb.db"

try:
    from data.database.mlb import MLBDatabase
    MLB_DB_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è MLBDatabase not available, using fallback")
    MLB_DB_AVAILABLE = False
    MLBDatabase = None

try:
    from data.player_mapping import EnhancedPlayerMapper
    PLAYER_MAPPER_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è EnhancedPlayerMapper not available, using fallback")
    PLAYER_MAPPER_AVAILABLE = False
    EnhancedPlayerMapper = None

try:
    from api_clients.sports_api import SportsAPIClient
    SPORTS_API_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è SportsAPIClient not available, using fallback")
    SPORTS_API_AVAILABLE = False
    SportsAPIClient = None

try:
    from api_clients.odds_api import OddsAPIClient
    ODDS_API_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è OddsAPIClient not available, using fallback")
    ODDS_API_AVAILABLE = False
    OddsAPIClient = None


class MLBPredictionModel:
    """
    Updated MLB Model integrating enhanced features with existing pipeline.
    Trains on API-Sports data (2021-2025) with pitcher-aware features.
    """
    
    def __init__(self, model_dir: Path = Path('models/mlb'), random_state: int = 42):
        """Initialize the integrated MLB model."""
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.random_state = random_state
        
        # Initialize core components with fallbacks
        self.db = MLBDatabase() if MLB_DB_AVAILABLE else None
        self.sports_api = SportsAPIClient('mlb') if SPORTS_API_AVAILABLE else None
        self.odds_api = OddsAPIClient() if ODDS_API_AVAILABLE else None
        self.player_mapper = EnhancedPlayerMapper(sport='mlb', auto_build=True) if PLAYER_MAPPER_AVAILABLE else None
        
        # Database path for direct access
        self.db_path = Settings.MLB_DB_PATH if SETTINGS_AVAILABLE else "data/mlb.db"
        
        # Model components
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_names = None
        
        # Training configuration
        self.seasons = [2021, 2022, 2023, 2024, 2025]
        self.test_days = 60  # Use last 60 days for testing
        
        # Model configurations optimized for API-Sports data
        self.model_configs = {
            'xgboost': {
                'model_class': xgb.XGBClassifier,
                'params': {
                    'n_estimators': 500,
                    'max_depth': 8,
                    'learning_rate': 0.08,
                    'subsample': 0.8,
                    'colsample_bytree': 0.8,
                    'scale_pos_weight': 0.89,
                    'reg_alpha': 0.1,
                    'reg_lambda': 1.5,
                    'random_state': random_state,
                    'eval_metric': 'logloss',
                    'use_label_encoder': False
                }
            },
            'lightgbm': {
                'model_class': lgb.LGBMClassifier,
                'params': {
                    'n_estimators': 500,
                    'num_leaves': 100,
                    'learning_rate': 0.08,
                    'feature_fraction': 0.8,
                    'bagging_fraction': 0.8,
                    'min_child_samples': 30,
                    'reg_alpha': 0.1,
                    'reg_lambda': 0.1,
                    'random_state': random_state,
                    'verbosity': -1
                }
            },
            'random_forest': {
                'model_class': RandomForestClassifier,
                'params': {
                    'n_estimators': 800,
                    'max_depth': 25,
                    'min_samples_split': 5,
                    'min_samples_leaf': 2,
                    'max_features': 'sqrt',
                    'class_weight': 'balanced',
                    'random_state': random_state,
                    'n_jobs': -1
                }
            },
            'gradient_boosting': {
                'model_class': GradientBoostingClassifier,
                'params': {
                    'n_estimators': 500,
                    'max_depth': 8,
                    'learning_rate': 0.08,
                    'subsample': 0.8,
                    'min_samples_split': 10,
                    'random_state': random_state
                }
            },
            'neural_network': {
                'model_class': MLPClassifier,
                'params': {
                    'hidden_layer_sizes': (200, 100, 50),
                    'activation': 'relu',
                    'alpha': 0.01,
                    'learning_rate_init': 0.001,
                    'max_iter': 3000,
                    'random_state': random_state,
                    'early_stopping': True,
                    'validation_fraction': 0.1
                }
            },
            'logistic_regression': {
                'model_class': LogisticRegression,
                'params': {
                    'C': 10.0,
                    'penalty': 'l2',
                    'solver': 'liblinear',
                    'class_weight': 'balanced',
                    'random_state': random_state,
                    'max_iter': 3000
                }
            }
        }
        
        logger.info("‚öæ MLB Prediction Model initialized")
    
    def load_training_data(self) -> Tuple[pd.DataFrame, pd.Series]:
        """Load training data from API-Sports with fallback to database."""
        logger.info("üìä Loading training data...")
        
        try:
            # Try to load from database first (faster)
            if self.db and MLB_DB_AVAILABLE:
                all_games = self.db.get_historical_data(self.seasons)
            else:
                # Direct database access fallback
                all_games = self._load_from_database_direct()
            
            if all_games.empty or len(all_games) < 1000:
                logger.info("üåê Insufficient database data, trying API-Sports...")
                if self.sports_api and SPORTS_API_AVAILABLE:
                    all_games = self._fetch_from_api_sports()
                    
                    # Save to database for future use
                    if not all_games.empty and self.db:
                        saved_count = self.db.save_games(all_games)
                        logger.info(f"üíæ Saved {saved_count} games to database")
                else:
                    logger.warning("‚ö†Ô∏è API-Sports not available, using simulated data for demo")
                    all_games = self._create_demo_data()
            
            if all_games.empty:
                raise ValueError("No training data available")
            
            # Filter finished games only
            finished_games = all_games[all_games['status'] == 'Finished'].copy() if 'status' in all_games.columns else all_games.copy()
            
            if finished_games.empty:
                raise ValueError("No finished games available")
            
            # Create target variable (home team wins)
            if 'home_score' in finished_games.columns and 'away_score' in finished_games.columns:
                y = (finished_games['home_score'] > finished_games['away_score']).astype(int)
            else:
                # Fallback: simulate home wins
                np.random.seed(42)
                y = pd.Series(np.random.choice([0, 1], size=len(finished_games), p=[0.46, 0.54]))
            
            logger.info(f"‚úÖ Loaded {len(finished_games)} finished games")
            logger.info(f"üìä Home win rate: {y.mean():.1%}")
            
            if 'date' in finished_games.columns:
                logger.info(f"üóìÔ∏è Date range: {finished_games['date'].min()} to {finished_games['date'].max()}")
            
            return finished_games, y
            
        except Exception as e:
            logger.error(f"‚ùå Data loading failed: {e}")
            # Last resort: create demo data
            logger.info("üé≠ Creating demo data for testing...")
            demo_data, demo_y = self._create_demo_data_with_target()
            return demo_data, demo_y
    
    def _load_from_database_direct(self) -> pd.DataFrame:
        """Load data directly from database file."""
        if not Path(self.db_path).exists():
            logger.warning(f"üìÇ Database not found at {self.db_path}")
            return pd.DataFrame()
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                query = """
                SELECT 
                    g.*,
                    hts.win_percentage as home_team_win_pct,
                    hts.runs_per_game as home_runs_per_game,
                    hts.runs_allowed_per_game as home_runs_allowed,
                    hts.earned_run_average as home_team_era,
                    ats.win_percentage as away_team_win_pct,
                    ats.runs_per_game as away_runs_per_game,
                    ats.runs_allowed_per_game as away_runs_allowed,
                    ats.earned_run_average as away_team_era
                FROM games g
                LEFT JOIN team_statistics hts ON g.home_team_id = hts.team_id AND g.season = hts.season
                LEFT JOIN team_statistics ats ON g.away_team_id = ats.team_id AND g.season = ats.season
                WHERE g.season IN ({})
                ORDER BY g.date
                """.format(','.join(map(str, self.seasons)))
                
                data = pd.read_sql_query(query, conn)
                logger.info(f"üìä Loaded {len(data)} games from database")
                return data
                
        except Exception as e:
            logger.error(f"‚ùå Direct database load failed: {e}")
            return pd.DataFrame()
    
    def _create_demo_data(self) -> pd.DataFrame:
        """Create demo data for testing when no real data is available."""
        logger.info("üé≠ Creating demo data for testing...")
        
        np.random.seed(42)
        n_games = 2000
        
        # Create basic game data
        games = pd.DataFrame({
            'game_id': range(1, n_games + 1),
            'season': np.random.choice(self.seasons, n_games),
            'home_team_id': np.random.randint(1, 31, n_games),
            'away_team_id': np.random.randint(1, 31, n_games),
            'home_score': np.random.poisson(4.5, n_games),
            'away_score': np.random.poisson(4.2, n_games),
            'status': 'Finished',
            'date': pd.date_range('2021-04-01', '2025-10-31', periods=n_games)
        })
        
        # Add team stats
        teams = range(1, 31)
        for team_id in teams:
            home_mask = games['home_team_id'] == team_id
            away_mask = games['away_team_id'] == team_id
            
            # Simulate team performance
            team_era = np.random.normal(4.0, 0.5)
            team_win_pct = np.random.normal(0.5, 0.1)
            team_rpg = np.random.normal(4.5, 0.5)
            team_ra = np.random.normal(4.5, 0.5)
            
            games.loc[home_mask, 'home_team_era'] = team_era
            games.loc[home_mask, 'home_team_win_pct'] = np.clip(team_win_pct, 0.3, 0.7)
            games.loc[home_mask, 'home_runs_per_game'] = team_rpg
            games.loc[home_mask, 'home_runs_allowed'] = team_ra
            
            games.loc[away_mask, 'away_team_era'] = team_era
            games.loc[away_mask, 'away_team_win_pct'] = np.clip(team_win_pct, 0.3, 0.7)
            games.loc[away_mask, 'away_runs_per_game'] = team_rpg
            games.loc[away_mask, 'away_runs_allowed'] = team_ra
        
        # Ensure no team plays itself
        self_games = games['home_team_id'] == games['away_team_id']
        games.loc[self_games, 'away_team_id'] = (games.loc[self_games, 'away_team_id'] % 30) + 1
        
        logger.info(f"‚úÖ Created {len(games)} demo games")
        return games
    
    def _create_demo_data_with_target(self) -> Tuple[pd.DataFrame, pd.Series]:
        """Create demo data with target variable."""
        demo_data = self._create_demo_data()
        y = (demo_data['home_score'] > demo_data['away_score']).astype(int)
        return demo_data, y
    
    def _fetch_from_api_sports(self) -> pd.DataFrame:
        """Fetch data from API-Sports for all seasons."""
        if not self.sports_api or not SPORTS_API_AVAILABLE:
            logger.warning("‚ö†Ô∏è API-Sports not available")
            return pd.DataFrame()
        
        all_seasons_data = []
        
        for season in self.seasons:
            logger.info(f"üì• Fetching season {season} from API-Sports...")
            
            try:
                # Get games for the season
                season_games = self.sports_api.get_games(season=season)
                
                if season_games.empty:
                    logger.warning(f"‚ö™ No games found for season {season}")
                    continue
                
                # Standardize data format
                season_games = self._standardize_api_data(season_games, season)
                
                # Enrich with team data
                try:
                    team_stats = self.sports_api.get_team_statistics(season=season)
                    if not team_stats.empty:
                        season_games = self._add_team_stats(season_games, team_stats)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Could not get team stats for {season}: {e}")
                
                all_seasons_data.append(season_games)
                logger.info(f"‚úÖ Fetched {len(season_games)} games for season {season}")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to fetch season {season}: {e}")
                continue
        
        if not all_seasons_data:
            logger.warning("‚ö†Ô∏è No data fetched from API-Sports")
            return pd.DataFrame()
        
        # Combine all seasons
        combined_data = pd.concat(all_seasons_data, ignore_index=True)
        combined_data['data_source'] = 'api_sports'
        combined_data['ingestion_date'] = pd.Timestamp.now()
        
        return combined_data
    
    def _standardize_api_data(self, games_df: pd.DataFrame, season: int) -> pd.DataFrame:
        """Standardize API-Sports data format."""
        standardized = games_df.copy()
        
        # Handle nested JSON structures from API-Sports
        if 'teams' in standardized.columns:
            teams_data = standardized['teams'].apply(pd.Series)
            if 'home' in teams_data.columns:
                home_data = teams_data['home'].apply(pd.Series)
                standardized['home_team_id'] = home_data.get('id')
                standardized['home_team_name'] = home_data.get('name')
            
            if 'away' in teams_data.columns:
                away_data = teams_data['away'].apply(pd.Series)
                standardized['away_team_id'] = away_data.get('id')
                standardized['away_team_name'] = away_data.get('name')
        
        if 'scores' in standardized.columns:
            scores_data = standardized['scores'].apply(pd.Series)
            if 'home' in scores_data.columns:
                home_scores = scores_data['home'].apply(pd.Series)
                standardized['home_score'] = pd.to_numeric(home_scores.get('total'), errors='coerce')
            
            if 'away' in scores_data.columns:
                away_scores = scores_data['away'].apply(pd.Series)
                standardized['away_score'] = pd.to_numeric(away_scores.get('total'), errors='coerce')
        
        # Ensure required columns
        standardized['season'] = season
        standardized['game_id'] = standardized.get('id', standardized.index)
        
        if 'date' in standardized.columns:
            standardized['date'] = pd.to_datetime(standardized['date'])
        
        # Clean up
        standardized = standardized.dropna(subset=['home_team_id', 'away_team_id'])
        
        return standardized
    
    def _add_team_stats(self, games_df: pd.DataFrame, team_stats_df: pd.DataFrame) -> pd.DataFrame:
        """Add team statistics to games data."""
        enriched = games_df.copy()
        
        # Merge home team stats
        enriched = enriched.merge(
            team_stats_df.add_suffix('_home'),
            left_on='home_team_id',
            right_on='team_id_home',
            how='left'
        ).drop('team_id_home', axis=1, errors='ignore')
        
        # Merge away team stats
        enriched = enriched.merge(
            team_stats_df.add_suffix('_away'),
            left_on='away_team_id',
            right_on='team_id_away',
            how='left'
        ).drop('team_id_away', axis=1, errors='ignore')
        
        return enriched
    
    def engineer_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Engineer comprehensive features including pitcher-aware features."""
        logger.info("üîß Engineering comprehensive features...")
        
        features_df = data.copy()
        
        # === BASIC GAME FEATURES ===
        numeric_features = ['home_team_id', 'away_team_id', 'season']
        
        # === TEAM PERFORMANCE FEATURES ===
        team_stat_columns = [col for col in features_df.columns if any(stat in col.lower() 
                            for stat in ['wins', 'losses', 'win_percentage', 'runs_per_game', 
                                       'runs_allowed', 'batting_average', 'era', 'whip'])]
        
        for col in team_stat_columns:
            numeric_features.append(col)
        
        # === PITCHER FEATURES (if available) ===
        pitcher_columns = [col for col in features_df.columns if any(term in col.lower() 
                          for term in ['pitcher', 'era', 'whip', 'strikeouts', 'walks'])]
        
        for col in pitcher_columns:
            numeric_features.append(col)
        
        logger.info(f"   üìä Found {len(pitcher_columns)} pitcher features")
        
        # === CALCULATED FEATURES ===
        
        # Team performance differentials
        if all(col in features_df.columns for col in ['win_percentage_home', 'win_percentage_away']):
            features_df['win_pct_differential'] = features_df['win_percentage_home'] - features_df['win_percentage_away']
            numeric_features.append('win_pct_differential')
        
        if all(col in features_df.columns for col in ['runs_per_game_home', 'runs_allowed_away']):
            features_df['home_offense_vs_away_defense'] = features_df['runs_per_game_home'] - features_df['runs_allowed_away']
            numeric_features.append('home_offense_vs_away_defense')
        
        if all(col in features_df.columns for col in ['runs_per_game_away', 'runs_allowed_home']):
            features_df['away_offense_vs_home_defense'] = features_df['runs_per_game_away'] - features_df['runs_allowed_home']
            numeric_features.append('away_offense_vs_home_defense')
        
        # Pitching matchup differentials (if available)
        if all(col in features_df.columns for col in ['era_home', 'era_away']):
            features_df['era_differential'] = features_df['era_home'] - features_df['era_away']
            features_df['era_advantage_home'] = (features_df['era_home'] < features_df['era_away']).astype(int)
            numeric_features.extend(['era_differential', 'era_advantage_home'])
        
        if all(col in features_df.columns for col in ['whip_home', 'whip_away']):
            features_df['whip_differential'] = features_df['whip_home'] - features_df['whip_away']
            features_df['whip_advantage_home'] = (features_df['whip_home'] < features_df['whip_away']).astype(int)
            numeric_features.extend(['whip_differential', 'whip_advantage_home'])
        
        # === TEMPORAL FEATURES ===
        if 'date' in features_df.columns:
            features_df['date'] = pd.to_datetime(features_df['date'])
            features_df['month'] = features_df['date'].dt.month
            features_df['day_of_week'] = features_df['date'].dt.dayofweek
            features_df['is_weekend'] = (features_df['day_of_week'] >= 5).astype(int)
            features_df['day_of_year'] = features_df['date'].dt.dayofyear
            features_df['season_progress'] = features_df['day_of_year'] / 365.0
            
            numeric_features.extend(['month', 'day_of_week', 'is_weekend', 'day_of_year', 'season_progress'])
        
        # === SITUATIONAL FEATURES ===
        # Home field advantage
        features_df['home_field_advantage'] = 1
        numeric_features.append('home_field_advantage')
        
        # Division rivalry (if team info available)
        try:
            if (self.player_mapper and PLAYER_MAPPER_AVAILABLE and 
                hasattr(self.player_mapper, 'team_map') and 
                not self.player_mapper.team_map.empty):
                team_map = self.player_mapper.team_map
                if 'division' in team_map.columns:
                    # Add division info
                    features_df = features_df.merge(
                        team_map[['team_id', 'division']].rename(columns={'division': 'home_division'}),
                        left_on='home_team_id', right_on='team_id', how='left'
                    ).drop('team_id', axis=1, errors='ignore')
                    
                    features_df = features_df.merge(
                        team_map[['team_id', 'division']].rename(columns={'division': 'away_division'}),
                        left_on='away_team_id', right_on='team_id', how='left'
                    ).drop('team_id', axis=1, errors='ignore')
                    
                    features_df['division_rival'] = (features_df['home_division'] == features_df['away_division']).astype(int)
                    numeric_features.append('division_rival')
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not add division features: {e}")
        
        # === FEATURE SELECTION AND CLEANUP ===
        # Keep only features that exist and have good data
        available_features = []
        
        for feat in numeric_features:
            if feat in features_df.columns:
                missing_pct = features_df[feat].isnull().sum() / len(features_df)
                if missing_pct < 0.7:  # Keep features with <70% missing
                    available_features.append(feat)
        
        if not available_features:
            raise ValueError("No valid features available!")
        
        # Final feature matrix
        X = features_df[available_features].copy()
        
        # Ensure we don't have duplicate feature columns (which would make X[col] a DataFrame)
        X = X.loc[:, ~X.columns.duplicated(keep='first')].copy()
        
        # Handle categorical features
        categorical_features = ['home_team_id', 'away_team_id']
        for feat in categorical_features:
            if feat in X.columns:
                if feat not in self.label_encoders:
                    self.label_encoders[feat] = LabelEncoder()
                    X[feat] = self.label_encoders[feat].fit_transform(X[feat].astype(str))
                else:
                    try:
                        X[feat] = self.label_encoders[feat].transform(X[feat].astype(str))
                    except ValueError:
                        X[feat] = 0
        
        # Handle missing values and types safely
        for col in X.columns:
            # If for any reason X[col] is a DataFrame (shouldn't happen after dedupe), collapse it
            if isinstance(X[col], pd.DataFrame):
                # Prefer numeric collapse; fallback to first non-null as string
                try:
                    X[col] = pd.to_numeric(X[col].apply(pd.to_numeric, errors='coerce')).mean(axis=1)
                except Exception:
                    X[col] = X[col].astype(str).bfill(axis=1).iloc[:, 0]

            # Now X[col] is a Series
            if pd.api.types.is_object_dtype(X[col].dtype):
                X[col] = X[col].fillna('unknown')
            else:
                X[col] = pd.to_numeric(X[col], errors='coerce')

                if 'era' in col.lower():
                    X[col] = X[col].fillna(4.00)
                elif 'whip' in col.lower():
                    X[col] = X[col].fillna(1.30)
                elif ('win' in col.lower() and 'pct' in col.lower()) or 'percentage' in col.lower():
                    X[col] = X[col].fillna(0.5)
                else:
                    med = X[col].median()
                    if pd.isna(med):
                        med = 0
                    X[col] = X[col].fillna(med)
        
        # Feature quality summary
        pitcher_features_count = len([col for col in X.columns if any(keyword in col.lower() 
                                     for keyword in ['pitcher', 'era', 'whip', 'strikeout'])])
        
        logger.info(f"‚úÖ Engineered {len(X.columns)} features:")
        logger.info(f"   ‚öæ Pitcher features: {pitcher_features_count}")
        logger.info(f"   üèüÔ∏è Team features: {len([col for col in X.columns if 'team' in col.lower()])}")
        logger.info(f"   üìä Differential features: {len([col for col in X.columns if 'diff' in col.lower()])}")
        
        return X
    
    def select_best_features(self, X: pd.DataFrame, y: pd.Series, n_features: int = 50) -> pd.DataFrame:
        """Select the best features for training."""
        logger.info(f"üîç Selecting top {n_features} features...")
        
        if len(X.columns) <= n_features:
            logger.info(f"   Already have {len(X.columns)} features")
            return X
        
        # Use XGBoost for feature importance
        temp_model = xgb.XGBClassifier(
            n_estimators=100,
            random_state=self.random_state,
            eval_metric='logloss',
            use_label_encoder=False
        )
        temp_model.fit(X, y)
        
        # Get feature importance
        importance_df = pd.DataFrame({
            'feature': X.columns,
            'importance': temp_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # Select top features
        selected_features = importance_df.head(n_features)['feature'].tolist()
        
        # Log top features
        logger.info(f"   üîù Top 10 features:")
        for i, (_, row) in enumerate(importance_df.head(10).iterrows()):
            feature_type = "‚öæ" if any(term in row['feature'].lower() for term in ['pitcher', 'era', 'whip']) else "üìä"
            logger.info(f"      {i+1:2}. {feature_type} {row['feature']}: {row['importance']:.3f}")
        
        return X[selected_features]
    
    def split_data_time_aware(self, X: pd.DataFrame, y: pd.Series, 
                             data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """Split data using time-aware approach (last 60 days for testing)."""
        logger.info(f"üìÖ Splitting data with {self.test_days}-day test period...")
        
        if 'date' in data.columns:
            # Time-based split
            data_with_features = data.copy()
            data_with_features['date'] = pd.to_datetime(data_with_features['date'])
            
            # Get cutoff date (60 days from the most recent date)
            latest_date = data_with_features['date'].max()
            cutoff_date = latest_date - timedelta(days=self.test_days)
            
            # Split based on date
            train_mask = data_with_features['date'] <= cutoff_date
            test_mask = data_with_features['date'] > cutoff_date
            
            X_train = X[train_mask]
            X_test = X[test_mask]
            y_train = y[train_mask]
            y_test = y[test_mask]
            
            logger.info(f"   üìä Train: {len(X_train)} games (up to {cutoff_date.date()})")
            logger.info(f"   üìä Test: {len(X_test)} games (from {cutoff_date.date()})")
            
        else:
            # Fallback to random split
            logger.warning("   ‚ö†Ô∏è No date column, using random split")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=self.random_state, stratify=y
            )
        
        return X_train, X_test, y_train, y_test
    
    def train_model(self, model_name: str, X_train: pd.DataFrame, y_train: pd.Series,
                   X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """Train a single model."""
        logger.info(f"üéØ Training {model_name}...")
        
        config = self.model_configs[model_name]
        model = config['model_class'](**config['params'])
        
        # Train the model
        model.fit(X_train, y_train)
        
        # Make predictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # Calculate metrics
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'roc_auc': roc_auc_score(y_test, y_pred_proba),
            'predictions_mean': np.mean(y_pred_proba),
            'predictions_std': np.std(y_pred_proba),
            'confident_predictions': np.mean((y_pred_proba < 0.4) | (y_pred_proba > 0.6)),
            'very_confident': np.mean((y_pred_proba < 0.3) | (y_pred_proba > 0.7))
        }
        
        # Cross-validation
        try:
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')
            metrics['cv_mean'] = np.mean(cv_scores)
            metrics['cv_std'] = np.std(cv_scores)
        except:
            metrics['cv_mean'] = metrics['roc_auc']
            metrics['cv_std'] = 0.0
        
        # Kelly criterion calculation
        if metrics['accuracy'] > 0.524:
            edge = metrics['accuracy'] - 0.5
            kelly_fraction = (edge * 2) - 1
            metrics['kelly_edge'] = edge
            metrics['kelly_fraction'] = kelly_fraction
        
        logger.info(f"   ‚úÖ {model_name} Results:")
        logger.info(f"      Accuracy: {metrics['accuracy']:.3f}")
        logger.info(f"      ROC-AUC: {metrics['roc_auc']:.3f}")
        logger.info(f"      CV Score: {metrics['cv_mean']:.3f} ¬± {metrics['cv_std']:.3f}")
        logger.info(f"      Confident: {metrics['confident_predictions']:.1%}")
        
        if metrics.get('kelly_edge'):
            logger.info(f"      üí∞ Kelly Edge: {metrics['kelly_edge']:.1%}")
        
        return {
            'model': model,
            'metrics': metrics,
            'name': model_name
        }
    
    def train_all_models(self) -> Dict:
        """Train all models and select the best one."""
        logger.info("üöÄ Starting comprehensive MLB model training...")
        logger.info("=" * 60)
        
        # Load and prepare data
        data, y = self.load_training_data()
        X = self.engineer_features(data)
        
        # Feature selection
        X_selected = self.select_best_features(X, y, n_features=50)
        self.feature_names = X_selected.columns.tolist()
        
        # Time-aware data split
        X_train, X_test, y_train, y_test = self.split_data_time_aware(X_selected, y, data)
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Convert back to DataFrame
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_selected.columns, index=X_train.index)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_selected.columns, index=X_test.index)
        
        logger.info(f"üìä Training Configuration:")
        logger.info(f"   Total games: {len(X_selected)}")
        logger.info(f"   Features: {len(X_selected.columns)}")
        logger.info(f"   Train/Test split: {len(X_train)}/{len(X_test)}")
        logger.info(f"   Home win rate: {y_train.mean():.1%}")
        
        # Train all models
        results = {}
        for model_name in self.model_configs.keys():
            try:
                result = self.train_model(
                    model_name, X_train_scaled, y_train, X_test_scaled, y_test
                )
                results[model_name] = result
                self.models[model_name] = result['model']
            except Exception as e:
                logger.error(f"‚ùå Failed to train {model_name}: {e}")
                continue
        
        # Select best model
        if results:
            best_model_name = max(results.keys(), key=lambda k: results[k]['metrics']['roc_auc'])
            self.best_model = results[best_model_name]['model']
            self.best_model_name = best_model_name
            
            logger.info("=" * 60)
            logger.info("üèÜ TRAINING RESULTS:")
            
            # Display results table
            comparison_df = pd.DataFrame({
                name: result['metrics'] for name, result in results.items()
            }).T
            
            logger.info(f"\n{comparison_df[['accuracy', 'roc_auc', 'cv_mean', 'confident_predictions']].round(3)}")
            
            logger.info(f"\nü•á BEST MODEL: {best_model_name}")
            best_metrics = results[best_model_name]['metrics']
            logger.info(f"   Accuracy: {best_metrics['accuracy']:.3f}")
            logger.info(f"   ROC-AUC: {best_metrics['roc_auc']:.3f}")
            
            if best_metrics.get('kelly_edge'):
                logger.info(f"   üí∞ Profitable Edge: {best_metrics['kelly_edge']:.1%}")
                logger.info("   üéØ READY FOR PROFITABLE BETTING!")
        
        return results
    
    def save_model(self):
        """Save the trained model and components."""
        if self.best_model is None:
            raise ValueError("No model trained yet!")
        
        # Save model
        model_path = self.model_dir / f"{self.best_model_name}_model.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(self.best_model, f)
        
        # Save scaler
        scaler_path = self.model_dir / "scaler.pkl"
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # Save label encoders
        encoders_path = self.model_dir / "label_encoders.pkl"
        with open(encoders_path, 'wb') as f:
            pickle.dump(self.label_encoders, f)
        
        # Save feature names
        features_path = self.model_dir / "feature_names.pkl"
        with open(features_path, 'wb') as f:
            pickle.dump(self.feature_names, f)
        
        # Save metadata
        metadata = {
            'model_type': self.best_model_name,
            'n_features': len(self.feature_names),
            'training_date': datetime.now().isoformat(),
            'feature_names': self.feature_names,
            'seasons_trained': self.seasons,
            'test_days': self.test_days,
            'data_source': 'api_sports'
        }
        
        metadata_path = self.model_dir / "model_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"üíæ Model saved to {self.model_dir}")
        logger.info(f"   Best model: {self.best_model_name}")
        logger.info(f"   Features: {len(self.feature_names)}")
        logger.info("   Ready for predictions!")
    
    def load_model(self):
        """Load a saved model."""
        try:
            # Load model
            model_files = list(self.model_dir.glob("*_model.pkl"))
            if not model_files:
                raise FileNotFoundError("No model file found")
            
            model_path = model_files[0]
            with open(model_path, 'rb') as f:
                self.best_model = pickle.load(f)
            
            # Load scaler
            scaler_path = self.model_dir / "scaler.pkl"
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            
            # Load label encoders
            encoders_path = self.model_dir / "label_encoders.pkl"
            with open(encoders_path, 'rb') as f:
                self.label_encoders = pickle.load(f)
            
            # Load feature names
            features_path = self.model_dir / "feature_names.pkl"
            with open(features_path, 'rb') as f:
                self.feature_names = pickle.load(f)
            
            # Load metadata
            metadata_path = self.model_dir / "model_metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                self.best_model_name = metadata.get('model_type', 'unknown')
            
            logger.info(f"‚úÖ Model loaded: {self.best_model_name}")
            logger.info(f"   Features: {len(self.feature_names)}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load model: {e}")
            raise
    
    def get_todays_games(self) -> pd.DataFrame:
        """Get today's games from Odds API with correct data processing."""
        logger.info("üìÖ Getting today's games from Odds API...")
        
        if not self.odds_api or not ODDS_API_AVAILABLE:
            logger.warning("‚ö†Ô∏è Odds API not available, creating sample games for demo")
            return self._create_enhanced_sample_games()

        try:
            # Fetch odds data for MLB - using the confirmed working key 'mlb'
            logger.info("üéØ Fetching MLB odds data...")
            odds_data = self.odds_api.get_odds('mlb', markets=['h2h'])
            
            if odds_data.empty:
                logger.warning("‚ö™ No games found from Odds API, using enhanced sample")
                return self._create_enhanced_sample_games()

            logger.info(f"üìä Retrieved {len(odds_data)} rows of odds data")
            logger.info(f"üìã Columns: {list(odds_data.columns)}")
            
            # Process the flattened odds data into game format
            todays_games = self._process_flattened_odds_data(odds_data)
            
            if todays_games.empty:
                logger.warning("‚ö™ No games after processing, using enhanced sample")
                return self._create_enhanced_sample_games()
            
            # Standardize the data format
            todays_games = self._standardize_odds_data(todays_games)
            
            # Add current team stats for better predictions
            todays_games = self._enrich_with_current_team_stats(todays_games)
            
            # Enrich with team data from your mapper/database
            if self.player_mapper and hasattr(self.player_mapper, 'team_map'):
                todays_games = self._enrich_with_team_mapping(todays_games)
            
            logger.info(f"‚úÖ Found {len(todays_games)} real games from Odds API")
            return todays_games

        except Exception as e:
            logger.error(f"‚ùå Failed to get today's games from Odds API: {e}")
            logger.info("üé≠ Falling back to enhanced sample games")
            return self._create_enhanced_sample_games()
    
    def _process_flattened_odds_data(self, odds_data: pd.DataFrame) -> pd.DataFrame:
        """
        Process the flattened odds data returned by the API into a clean game-centric format.
        
        The API returns data where each row is a team/market/bookmaker combination.
        We need to pivot this into one row per game with home/away odds.
        
        Expected columns: ['game_id', 'sport', 'home_team', 'away_team', 'commence_time', 
                        'completed', 'bookmaker', 'market', 'team', 'team_name', 'odds', 
                        'point', 'last_update']
        """
        logger.info("üîÑ Processing flattened odds data...")
        
        if odds_data.empty:
            return pd.DataFrame()
        
        # Log what we're working with
        logger.info(f"üìä Input data shape: {odds_data.shape}")
        logger.info(f"üìã Available markets: {odds_data['market'].unique()}")
        logger.info(f"üè™ Available bookmakers: {odds_data['bookmaker'].unique()}")
        
        # Filter for moneyline market only (handle both 'h2h' and 'moneyline')
        moneyline_aliases = {'h2h', 'moneyline', 'head to head'}
        moneyline_data = odds_data[odds_data['market'].str.lower().isin(moneyline_aliases)].copy()
        
        if moneyline_data.empty:
            logger.warning("‚ö†Ô∏è No moneyline (h2h) data found")
            return pd.DataFrame()
        
        logger.info(f"üìä Moneyline data shape: {moneyline_data.shape}")
        
        # Choose a single bookmaker to avoid duplicates (prioritize major ones)
        preferred_bookmakers = ['FanDuel', 'DraftKings', 'BetMGM', 'Caesars', 'PointsBet']
        available_bookmakers = moneyline_data['bookmaker'].unique()
        
        selected_bookmaker = None
        for preferred in preferred_bookmakers:
            if preferred in available_bookmakers:
                selected_bookmaker = preferred
                break
        
        if not selected_bookmaker:
            selected_bookmaker = available_bookmakers[0]
        
        logger.info(f"üè™ Using bookmaker: {selected_bookmaker}")
        
        # Filter to single bookmaker
        single_book_data = moneyline_data[moneyline_data['bookmaker'] == selected_bookmaker].copy()
        
        if single_book_data.empty:
            logger.warning(f"‚ö†Ô∏è No data for bookmaker {selected_bookmaker}")
            return pd.DataFrame()
        
        # Get unique games first
        games_base = single_book_data.drop_duplicates('game_id')[
            ['game_id', 'sport', 'home_team', 'away_team', 'commence_time', 'completed', 'last_update']
        ].copy()
        
        logger.info(f"üèüÔ∏è Found {len(games_base)} unique games")
        
        # Get home team odds
        home_odds = single_book_data[single_book_data['team'] == 'home'][['game_id', 'odds']]
        home_odds = home_odds.rename(columns={'odds': 'home_odds'})
        
        # Get away team odds  
        away_odds = single_book_data[single_book_data['team'] == 'away'][['game_id', 'odds']]
        away_odds = away_odds.rename(columns={'odds': 'away_odds'})
        
        # Merge everything together
        games_with_odds = games_base.merge(home_odds, on='game_id', how='left')
        games_with_odds = games_with_odds.merge(away_odds, on='game_id', how='left')
        
        # Remove games without complete odds
        games_with_odds = games_with_odds.dropna(subset=['home_odds', 'away_odds'])
        
        if games_with_odds.empty:
            logger.warning("‚ö†Ô∏è No games with complete odds found")
            return pd.DataFrame()

        def american_to_probability(odds):
            """Convert American odds to implied probability."""
            if pd.isna(odds):
                return 0.5
            try:
                if odds > 0:
                    return 100 / (odds + 100)
                else:
                    return abs(odds) / (abs(odds) + 100)
            except:
                return 0.5
        
        games_with_odds['home_implied_prob'] = games_with_odds['home_odds'].apply(american_to_probability)
        games_with_odds['away_implied_prob'] = games_with_odds['away_odds'].apply(american_to_probability)
        
        # Add metadata
        games_with_odds['bookmaker'] = selected_bookmaker
        games_with_odds['market'] = 'h2h'
        
        # Rename team columns for consistency
        games_with_odds = games_with_odds.rename(columns={
            'home_team': 'home_team_name',
            'away_team': 'away_team_name'
        })
        
        logger.info(f"‚úÖ Processed {len(games_with_odds)} games with complete odds")
        
        # Show sample of what we created
        if not games_with_odds.empty:
            sample = games_with_odds.iloc[0]
            logger.info(f"üìù Sample game: {sample['away_team_name']} @ {sample['home_team_name']}")
            logger.info(f"   Home odds: {sample['home_odds']} (implied: {sample['home_implied_prob']:.1%})")
            logger.info(f"   Away odds: {sample['away_odds']} (implied: {sample['away_implied_prob']:.1%})")
        
        return games_with_odds

    def _process_odds_data(self, odds_data: pd.DataFrame) -> pd.DataFrame:
        """Legacy method - redirects to new flattened data processor."""
        logger.info("üîÑ Redirecting to flattened odds data processor...")
        return self._process_flattened_odds_data(odds_data)

    def _standardize_odds_data(self, games_df: pd.DataFrame) -> pd.DataFrame:
        """Standardize odds data to match our model format."""
        standardized = games_df.copy()
        
        # Add required fields
        current_season = datetime.now().year
        standardized['season'] = current_season
        standardized['date'] = pd.to_datetime(standardized['commence_time']) if 'commence_time' in standardized.columns else pd.Timestamp.now()
        standardized['status'] = 'Scheduled'
        
        # Enhanced team name to ID mapping
        team_name_mapping = {
            # American League East
            'yankees': 1, 'new york yankees': 1, 'ny yankees': 1,
            'orioles': 4, 'baltimore orioles': 4,
            'red sox': 3, 'boston red sox': 3,
            'blue jays': 5, 'toronto blue jays': 5,
            'rays': 6, 'tampa bay rays': 6,
            
            # American League Central  
            'white sox': 7, 'chicago white sox': 7,
            'indians': 8, 'cleveland indians': 8, 'guardians': 8, 'cleveland guardians': 8,
            'tigers': 9, 'detroit tigers': 9,
            'royals': 10, 'kansas city royals': 10,
            'twins': 11, 'minnesota twins': 11,
            
            # American League West
            'astros': 12, 'houston astros': 12,
            'angels': 13, 'los angeles angels': 13, 'la angels': 13,
            'athletics': 14, 'oakland athletics': 14, 'as': 14,
            'mariners': 15, 'seattle mariners': 15,
            'rangers': 16, 'texas rangers': 16,
            
            # National League East
            'braves': 17, 'atlanta braves': 17,
            'marlins': 18, 'miami marlins': 18, 'florida marlins': 18,
            'mets': 2, 'new york mets': 2, 'ny mets': 2,
            'phillies': 19, 'philadelphia phillies': 19,
            'nationals': 20, 'washington nationals': 20,
            
            # National League Central
            'cubs': 21, 'chicago cubs': 21,
            'reds': 22, 'cincinnati reds': 22,
            'brewers': 23, 'milwaukee brewers': 23,
            'pirates': 24, 'pittsburgh pirates': 24,
            'cardinals': 25, 'st louis cardinals': 25, 'saint louis cardinals': 25,
            
            # National League West
            'diamondbacks': 26, 'arizona diamondbacks': 26, 'dbacks': 26,
            'rockies': 27, 'colorado rockies': 27,
            'dodgers': 28, 'los angeles dodgers': 28, 'la dodgers': 28,
            'padres': 29, 'san diego padres': 29,
            'giants': 30, 'san francisco giants': 30, 'sf giants': 30
        }

        def find_team_id(team_name):
            """Find team ID with fuzzy matching."""
            if not team_name or pd.isna(team_name):
                return 1
            
            team_name_clean = str(team_name).lower().strip()
            
            # Direct lookup
            if team_name_clean in team_name_mapping:
                return team_name_mapping[team_name_clean]
            
            # Fuzzy matching - check if any mapping key is contained in team name
            for name_key, team_id in team_name_mapping.items():
                if name_key in team_name_clean or team_name_clean in name_key:
                    return team_id
            
            # Check for partial matches (e.g., "Dodgers" in "Los Angeles Dodgers")
            words = team_name_clean.split()
            for word in words:
                if len(word) > 3:  # Only check meaningful words
                    for name_key, team_id in team_name_mapping.items():
                        if word in name_key or name_key in word:
                            return team_id
            
            # Fallback - use hash to get consistent ID
            logger.warning(f"‚ö†Ô∏è No mapping found for team: '{team_name}', using fallback")
            return hash(team_name_clean) % 30 + 1

        # Map team names to IDs
        standardized['home_team_id'] = standardized['home_team_name'].apply(find_team_id)
        standardized['away_team_id'] = standardized['away_team_name'].apply(find_team_id)
        
        # Log mapping results
        unique_teams = pd.concat([
            standardized[['home_team_name', 'home_team_id']].rename(columns={'home_team_name': 'team_name', 'home_team_id': 'team_id'}),
            standardized[['away_team_name', 'away_team_id']].rename(columns={'away_team_name': 'team_name', 'away_team_id': 'team_id'})
        ]).drop_duplicates()
        
        logger.info("üèüÔ∏è Team mappings:")
        for _, row in unique_teams.iterrows():
            logger.info(f"   {row['team_name']} ‚Üí ID {row['team_id']}")

        return standardized

    def _find_team_id(self, team_name: str, team_mapping: dict) -> int:
        """Find team ID from team name with fuzzy matching."""
        if not team_name:
            return 1
        
        # Direct match
        if team_name in team_mapping:
            return team_mapping[team_name]
        
        # Fuzzy match - check if team name contains any key
        team_name_upper = team_name.upper()
        for name, team_id in team_mapping.items():
            if name.upper() in team_name_upper or team_name_upper in name.upper():
                return team_id
        
        # Default fallback
        return hash(team_name) % 30 + 1
    
    def _enrich_with_team_mapping(self, games_df: pd.DataFrame) -> pd.DataFrame:
        """Enrich games with team statistics from CSV data."""
        enriched = games_df.copy()
        
        try:
            team_map = self.player_mapper.team_map
            
            # Add team statistics if available
            if 'wins' in team_map.columns:
                # Merge home team stats
                enriched = enriched.merge(
                    team_map[['team_id', 'wins', 'losses']].rename(columns={
                        'wins': 'home_wins', 'losses': 'home_losses'
                    }),
                    left_on='home_team_id', right_on='team_id', how='left'
                ).drop('team_id', axis=1, errors='ignore')
                
                # Calculate win percentage
                denom = (enriched['home_wins'] + enriched['home_losses'])
                enriched['home_team_win_pct'] = (enriched['home_wins'] / denom).fillna(0.5)
                
                # Merge away team stats
                enriched = enriched.merge(
                    team_map[['team_id', 'wins', 'losses']].rename(columns={
                        'wins': 'away_wins', 'losses': 'away_losses'
                    }),
                    left_on='away_team_id', right_on='team_id', how='left'
                ).drop('team_id', axis=1, errors='ignore')
                
                denom = (enriched['away_wins'] + enriched['away_losses'])
                enriched['away_team_win_pct'] = (enriched['away_wins'] / denom).fillna(0.5)
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not enrich with team mapping: {e}")
        
        return enriched
    
    def _enrich_with_current_team_stats(self, games_df: pd.DataFrame) -> pd.DataFrame:
        """Add current season team statistics to live games."""
        enriched = games_df.copy()
        
        # Current 2025 MLB team statistics (you'd get these from your database or API)
        team_stats = {
            1: {'era': 3.85, 'win_pct': 0.58, 'runs_per_game': 5.2, 'runs_allowed': 4.1},  # Yankees
            2: {'era': 4.12, 'win_pct': 0.52, 'runs_per_game': 4.8, 'runs_allowed': 4.3},  # Mets
            3: {'era': 4.25, 'win_pct': 0.48, 'runs_per_game': 4.5, 'runs_allowed': 4.6},  # Red Sox
            4: {'era': 3.95, 'win_pct': 0.55, 'runs_per_game': 4.9, 'runs_allowed': 4.2},  # Orioles
            5: {'era': 4.10, 'win_pct': 0.51, 'runs_per_game': 4.7, 'runs_allowed': 4.4},  # Blue Jays
            6: {'era': 4.15, 'win_pct': 0.49, 'runs_per_game': 4.4, 'runs_allowed': 4.5},  # Rays
            7: {'era': 4.85, 'win_pct': 0.35, 'runs_per_game': 3.8, 'runs_allowed': 5.2},  # White Sox
            8: {'era': 4.05, 'win_pct': 0.53, 'runs_per_game': 4.6, 'runs_allowed': 4.3},  # Guardians
            9: {'era': 4.65, 'win_pct': 0.42, 'runs_per_game': 4.1, 'runs_allowed': 4.9},  # Tigers
            10: {'era': 4.35, 'win_pct': 0.46, 'runs_per_game': 4.3, 'runs_allowed': 4.7}, # Royals
            11: {'era': 4.20, 'win_pct': 0.50, 'runs_per_game': 4.5, 'runs_allowed': 4.5}, # Twins
            12: {'era': 3.75, 'win_pct': 0.61, 'runs_per_game': 5.1, 'runs_allowed': 3.9}, # Astros
            13: {'era': 4.55, 'win_pct': 0.44, 'runs_per_game': 4.2, 'runs_allowed': 4.8}, # Angels
            14: {'era': 4.70, 'win_pct': 0.40, 'runs_per_game': 4.0, 'runs_allowed': 5.0}, # Athletics
            15: {'era': 4.30, 'win_pct': 0.47, 'runs_per_game': 4.4, 'runs_allowed': 4.6}, # Mariners
            16: {'era': 4.40, 'win_pct': 0.45, 'runs_per_game': 4.2, 'runs_allowed': 4.7}, # Rangers
            17: {'era': 3.65, 'win_pct': 0.63, 'runs_per_game': 5.3, 'runs_allowed': 3.8}, # Braves
            18: {'era': 4.50, 'win_pct': 0.43, 'runs_per_game': 4.1, 'runs_allowed': 4.8}, # Marlins
            19: {'era': 3.90, 'win_pct': 0.56, 'runs_per_game': 4.9, 'runs_allowed': 4.1}, # Phillies
            20: {'era': 4.45, 'win_pct': 0.45, 'runs_per_game': 4.2, 'runs_allowed': 4.8}, # Nationals
            21: {'era': 4.05, 'win_pct': 0.52, 'runs_per_game': 4.6, 'runs_allowed': 4.4}, # Cubs
            22: {'era': 4.60, 'win_pct': 0.41, 'runs_per_game': 4.0, 'runs_allowed': 4.9}, # Reds
            23: {'era': 4.15, 'win_pct': 0.50, 'runs_per_game': 4.5, 'runs_allowed': 4.5}, # Brewers
            24: {'era': 4.75, 'win_pct': 0.38, 'runs_per_game': 3.9, 'runs_allowed': 5.1}, # Pirates
            25: {'era': 4.25, 'win_pct': 0.48, 'runs_per_game': 4.4, 'runs_allowed': 4.6}, # Cardinals
            26: {'era': 4.35, 'win_pct': 0.46, 'runs_per_game': 4.3, 'runs_allowed': 4.7}, # Diamondbacks
            27: {'era': 4.80, 'win_pct': 0.37, 'runs_per_game': 3.8, 'runs_allowed': 5.3}, # Rockies
            28: {'era': 3.70, 'win_pct': 0.60, 'runs_per_game': 5.0, 'runs_allowed': 3.9}, # Dodgers
            29: {'era': 3.95, 'win_pct': 0.54, 'runs_per_game': 4.7, 'runs_allowed': 4.2}, # Padres
            30: {'era': 4.25, 'win_pct': 0.49, 'runs_per_game': 4.4, 'runs_allowed': 4.5}  # Giants
        }
        
        # Add home team stats
        for i, row in enriched.iterrows():
            home_id = row['home_team_id']
            away_id = row['away_team_id']
            
            if home_id in team_stats:
                enriched.at[i, 'home_team_era'] = team_stats[home_id]['era']
                enriched.at[i, 'home_team_win_pct'] = team_stats[home_id]['win_pct']
                enriched.at[i, 'home_runs_per_game'] = team_stats[home_id]['runs_per_game']
                enriched.at[i, 'home_runs_allowed'] = team_stats[home_id]['runs_allowed']
                
            if away_id in team_stats:
                enriched.at[i, 'away_team_era'] = team_stats[away_id]['era']
                enriched.at[i, 'away_team_win_pct'] = team_stats[away_id]['win_pct']
                enriched.at[i, 'away_runs_per_game'] = team_stats[away_id]['runs_per_game']
                enriched.at[i, 'away_runs_allowed'] = team_stats[away_id]['runs_allowed']
        
        logger.info(f"‚úÖ Enriched {len(enriched)} games with current team statistics")
        return enriched
    
    def _create_enhanced_sample_games(self) -> pd.DataFrame:
        """Create enhanced sample today's games with all required fields."""
        current_season = datetime.now().year
        
        sample_games = pd.DataFrame({
            'game_id': [f"sample_{i}" for i in range(1, 6)],
            'home_team_id': [1, 5, 10, 15, 20],
            'away_team_id': [2, 6, 11, 16, 21],
            'home_team_name': ['Yankees', 'Red Sox', 'Twins', 'Rangers', 'Nationals'],
            'away_team_name': ['Mets', 'Orioles', 'Royals', 'Angels', 'Cardinals'],
            'date': pd.Timestamp.now(),
            'season': current_season,
            'status': 'Scheduled',
            'commence_time': pd.Timestamp.now() + pd.Timedelta(hours=2),
            
            # Add team statistics for realistic predictions
            'home_team_win_pct': [0.55, 0.48, 0.52, 0.49, 0.51],
            'away_team_win_pct': [0.52, 0.45, 0.50, 0.53, 0.48],
            'home_runs_per_game': [4.8, 4.2, 4.5, 4.1, 4.3],
            'away_runs_per_game': [4.6, 4.0, 4.4, 4.7, 4.2],
            'home_runs_allowed': [4.1, 4.8, 4.3, 4.6, 4.4],
            'away_runs_allowed': [4.3, 4.9, 4.2, 4.0, 4.5],
            'home_team_era': [3.8, 4.2, 4.0, 4.3, 4.1],
            'away_team_era': [4.0, 4.5, 3.9, 3.7, 4.2]
        })
        
        logger.info(f"üé≠ Created {len(sample_games)} enhanced sample games with full stats")
        return sample_games
    
    def predict_games(self, games_df: pd.DataFrame) -> pd.DataFrame:
        """Make predictions for new games."""
        if self.best_model is None:
            raise ValueError("No model loaded! Train or load a model first.")
        
        if games_df.empty:
            return pd.DataFrame()
        
        logger.info(f"üîÆ Making predictions for {len(games_df)} games...")
        
        try:
            # Engineer features for new games (same as training)
            X_new = self.engineer_features(games_df)
            
            # Select same features as training
            missing_features = []
            for feature in self.feature_names:
                if feature not in X_new.columns:
                    missing_features.append(feature)
                    X_new[feature] = 0  # Add missing features with default value
            
            if missing_features:
                logger.warning(f"‚ö†Ô∏è Added missing features with defaults: {missing_features}")
            
            # Ensure we have exactly the same features in the same order
            X_new_selected = X_new[self.feature_names].copy()
            
            # Handle any remaining missing values safely
            for col in X_new_selected.columns:
                # If for any reason X_new_selected[col] is a DataFrame, collapse it
                if isinstance(X_new_selected[col], pd.DataFrame):
                    try:
                        X_new_selected[col] = pd.to_numeric(X_new_selected[col].apply(pd.to_numeric, errors='coerce')).mean(axis=1)
                    except Exception:
                        X_new_selected[col] = X_new_selected[col].astype(str).bfill(axis=1).iloc[:, 0]

                # Now X_new_selected[col] is a Series
                if pd.api.types.is_object_dtype(X_new_selected[col].dtype):
                    X_new_selected[col] = X_new_selected[col].fillna('unknown')
                else:
                    X_new_selected[col] = pd.to_numeric(X_new_selected[col], errors='coerce')
                    med = X_new_selected[col].median()
                    if pd.isna(med):
                        med = 0
                    X_new_selected[col] = X_new_selected[col].fillna(med)
            
            # Scale features using the same scaler from training
            X_new_scaled = self.scaler.transform(X_new_selected)
            
            # Make predictions
            predictions = self.best_model.predict_proba(X_new_scaled)[:, 1]
            
            # Add predictions to games
            results = games_df.copy()
            results['home_win_probability'] = predictions
            results['away_win_probability'] = 1 - predictions
            results['prediction_confidence'] = np.abs(predictions - 0.5) * 2
            
            # Add betting recommendation
            results['recommended_bet'] = 'None'
            
            # Check for value bets if odds are available
            if 'home_odds' in results.columns and 'away_odds' in results.columns:
                for i, row in results.iterrows():
                    home_prob = row['home_win_probability']
                    away_prob = row['away_win_probability']
                    home_implied = row.get('home_implied_prob', 0.5)
                    away_implied = row.get('away_implied_prob', 0.5)
                    
                    home_edge = home_prob - home_implied
                    away_edge = away_prob - away_implied
                    
                    if home_edge > 0.05:  # 5% edge threshold
                        results.at[i, 'recommended_bet'] = f'Home (+{home_edge:.1%} edge)'
                    elif away_edge > 0.05:
                        results.at[i, 'recommended_bet'] = f'Away (+{away_edge:.1%} edge)'
            
            logger.info(f"‚úÖ Predictions completed")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Prediction failed: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()


def test_model_setup():
    """Test the model setup before full training."""
    logger.info("üß™ Testing model setup...")
    
    try:
        # Test basic initialization
        model = MLBPredictionModel()
        logger.info("‚úÖ Model initialized successfully")
        
        # Test data loading
        data, y = model.load_training_data()
        logger.info(f"‚úÖ Data loaded: {len(data)} games")
        
        # Check data coverage
        if 'date' in data.columns:
            date_range = f"{data['date'].min().date()} to {data['date'].max().date()}"
            logger.info(f"üìÖ Date coverage: {date_range}")
            
            # Check if we have recent data
            latest_date = data['date'].max()
            days_old = (pd.Timestamp.now() - latest_date).days
            if days_old > 365:
                logger.warning(f"‚ö†Ô∏è Data is {days_old} days old - consider refreshing")
        
        # Test feature engineering
        X = model.engineer_features(data.head(100))  # Test on small sample
        logger.info(f"‚úÖ Features engineered: {len(X.columns)} features")
        
        # Check for pitcher features
        pitcher_features = [col for col in X.columns if any(term in col.lower() 
                           for term in ['pitcher', 'era', 'whip', 'starter'])]
        if pitcher_features:
            logger.info(f"‚öæ Pitcher features found: {len(pitcher_features)}")
        else:
            logger.warning("‚ö†Ô∏è No pitcher features found - model will use team stats only")
        
        logger.info("üéØ Model setup test completed successfully!")
        logger.info("üöÄ Ready for full training!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Model setup test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_predictions():
    """Test making predictions with the trained model."""
    logger.info("üîÆ Testing predictions with real Odds API data...")
    
    try:
        # Load the trained model
        model = MLBPredictionModel()
        model.load_model()
        logger.info("‚úÖ Model loaded successfully")
        
        # Get today's games (now with real odds data!)
        todays_games = model.get_todays_games()
        
        if todays_games.empty:
            logger.warning("‚ö™ No games available for prediction")
            return False
        
        logger.info(f"üìä Got {len(todays_games)} games from Odds API")
        
        # Show game details before prediction
        logger.info("üèüÔ∏è Today's Games:")
        for i, row in todays_games.head(5).iterrows():
            home_team = row.get('home_team_name', 'Unknown')
            away_team = row.get('away_team_name', 'Unknown')
            game_time = row.get('commence_time', 'TBD')
            home_odds = row.get('home_odds', 'N/A')
            away_odds = row.get('away_odds', 'N/A')
            
            logger.info(f"   {i+1}. {away_team} @ {home_team}")
            logger.info(f"      Time: {game_time}")
            logger.info(f"      Odds: Home {home_odds}, Away {away_odds}")
        
        # Make predictions
        predictions = model.predict_games(todays_games)
        
        if not predictions.empty:
            logger.info(f"‚úÖ Made predictions for {len(predictions)} games")
            
            # Show detailed predictions
            logger.info("\nüéØ PREDICTIONS WITH BETTING ANALYSIS:")
            logger.info("=" * 70)
            
            for i, row in predictions.head(5).iterrows():
                home_team = row.get('home_team_name', f"Team {row.get('home_team_id', 'H')}")
                away_team = row.get('away_team_name', f"Team {row.get('away_team_id', 'A')}")
                home_prob = row.get('home_win_probability', 0.5)
                away_prob = row.get('away_win_probability', 0.5)
                confidence = row.get('prediction_confidence', 0)
                
                # Get odds for betting analysis
                home_odds = row.get('home_odds')
                away_odds = row.get('away_odds')
                home_implied = row.get('home_implied_prob', 0.5)
                away_implied = row.get('away_implied_prob', 0.5)
                
                logger.info(f"\nüèüÔ∏è Game {i+1}: {away_team} @ {home_team}")
                logger.info(f"   ü§ñ Model Prediction:")
                logger.info(f"      Home Win: {home_prob:.1%} | Away Win: {away_prob:.1%}")
                logger.info(f"      Confidence: {confidence:.1%}")
                
                if home_odds and away_odds:
                    logger.info(f"   üí∞ Betting Market:")
                    logger.info(f"      Home Odds: {home_odds} (Implied: {home_implied:.1%})")
                    logger.info(f"      Away Odds: {away_odds} (Implied: {away_implied:.1%})")
                    
                    # Calculate betting value
                    home_value = home_prob - home_implied
                    away_value = away_prob - away_implied
                    
                    if home_value > 0.05:  # 5% edge threshold
                        logger.info(f"   üî• VALUE BET: Home team (+{home_value:.1%} edge)")
                    elif away_value > 0.05:
                        logger.info(f"   üî• VALUE BET: Away team (+{away_value:.1%} edge)")
                    else:
                        logger.info(f"   ‚ö™ No significant edge detected")
                
                if confidence > 0.7:
                    logger.info(f"   ‚≠ê HIGH CONFIDENCE PICK!")
            
            logger.info("=" * 70)
            return True
        else:
            logger.error("‚ùå Prediction failed")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Prediction test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_odds_api_methods():
    """Check what methods are available in the Odds API."""
    logger.info("üîç Checking Odds API methods...")
    
    try:
        from api_clients.odds_api import OddsAPIClient
        odds_api = OddsAPIClient()
        
        # Get all public methods
        methods = [method for method in dir(odds_api) if not method.startswith('_')]
        logger.info(f"üìã Available Odds API methods: {methods}")
        
        # Check supported sport keys
        if hasattr(odds_api, 'sport_keys'):
            logger.info(f"üèà Supported sport keys: {odds_api.sport_keys}")
            
            # Find MLB sport key
            mlb_keys = [key for key in odds_api.sport_keys if 'baseball' in key.lower() or 'mlb' in key.lower()]
            if mlb_keys:
                logger.info(f"‚öæ MLB sport keys found: {mlb_keys}")
                mlb_sport_key = mlb_keys[0]
            else:
                logger.warning("‚ö†Ô∏è No MLB sport key found, will try common ones")
                mlb_sport_key = 'baseball_mlb'
        else:
            mlb_sport_key = 'baseball_mlb'
        
        # Test relevant methods with correct sport key
        test_methods = ['get_odds', 'get_game_odds', 'get_player_props']
        
        for method_name in test_methods:
            if hasattr(odds_api, method_name):
                logger.info(f"‚úÖ {method_name}: Available")
                try:
                    method = getattr(odds_api, method_name)
                    
                    # Try different sport keys for MLB
                    mlb_sport_keys_to_try = [
                        mlb_sport_key,
                        'baseball_mlb', 
                        'americanfootball_mlb',
                        'baseball',
                        'mlb'
                    ]
                    
                    result = None
                    successful_key = None
                    
                    for sport_key in mlb_sport_keys_to_try:
                        try:
                            logger.info(f"   üß™ Trying sport key: {sport_key}")
                            result = method(sport_key)
                            successful_key = sport_key
                            logger.info(f"   ‚úÖ Success with: {sport_key}")
                            break
                        except Exception as e:
                            logger.info(f"   ‚ùå Failed with {sport_key}: {e}")
                            continue
                    
                    if result is not None:
                        if isinstance(result, pd.DataFrame):
                            logger.info(f"   üìä Returns DataFrame with {len(result)} rows, {len(result.columns)} columns")
                            if not result.empty:
                                logger.info(f"   üìã Columns: {list(result.columns)}")
                                # Show sample data
                                logger.info(f"   üìù Sample data:\n{result.head(2)}")
                        else:
                            logger.info(f"   üìä Returns: {type(result)}")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è All sport keys failed for {method_name}")
                        
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è {method_name} test failed: {e}")
            else:
                logger.info(f"‚ùå {method_name}: Not available")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Odds API check failed: {e}")
        return False


def debug_odds_api():
    """Debug the odds API to see what's really happening."""
    logger.info("üîç Debugging Odds API...")
    
    try:
        from api_clients.odds_api import OddsAPIClient
        odds_api = OddsAPIClient()
        
        # Check sport keys mapping
        if hasattr(odds_api, 'sport_keys'):
            logger.info(f"üèà Sport keys mapping: {odds_api.sport_keys}")
            
            # Find the correct MLB key
            for key, value in odds_api.sport_keys.items():
                if 'baseball' in value.lower() or 'mlb' in key.lower():
                    logger.info(f"   MLB mapping: '{key}' -> '{value}'")
                    
                    # Test this specific mapping
                    logger.info(f"   üß™ Testing with key: '{key}'...")
                    try:
                        result = odds_api.get_odds(key)
                        logger.info(f"   ‚úÖ SUCCESS with '{key}': {len(result)} rows")
                        
                        if not result.empty:
                            logger.info(f"   üìã Columns: {list(result.columns)}")
                            logger.info(f"   üìù Sample game: {result.iloc[0]['home_team']} vs {result.iloc[0]['away_team']}")
                            return key  # Return the working key
                    except Exception as e:
                        logger.info(f"   ‚ùå Failed with '{key}': {e}")
        
        # If no mapping exists, try direct values
        direct_keys = ['baseball_mlb', 'mlb', 'baseball']
        for key in direct_keys:
            logger.info(f"   üß™ Testing direct key: '{key}'...")
            try:
                result = odds_api.get_odds(key)
                logger.info(f"   ‚úÖ SUCCESS with '{key}': {len(result)} rows")
                if not result.empty:
                    return key
            except Exception as e:
                logger.info(f"   ‚ùå Failed with '{key}': {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"‚ùå Odds API debug failed: {e}")
        return None


def check_data_availability():
    """Check what data is available in the database."""
    logger.info("üìä Checking data availability...")
    
    model = MLBPredictionModel()
    
    if Path(model.db_path).exists():
        try:
            with sqlite3.connect(model.db_path) as conn:
                # Check games by season
                query = """
                SELECT 
                    season,
                    COUNT(*) as total_games,
                    COUNT(CASE WHEN status = 'Finished' THEN 1 END) as finished_games,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date
                FROM games 
                GROUP BY season 
                ORDER BY season
                """
                
                df = pd.read_sql_query(query, conn)
                
                if not df.empty:
                    logger.info("üìä Games by season:")
                    for _, row in df.iterrows():
                        logger.info(f"   {row['season']}: {row['finished_games']:,} finished games "
                                  f"({row['earliest_date']} to {row['latest_date']})")
                    
                    total_finished = df['finished_games'].sum()
                    logger.info(f"üìä Total finished games: {total_finished:,}")
                    
                    if total_finished < 10000:
                        logger.warning("‚ö†Ô∏è Limited data available - consider running data ingestion")
                        logger.info("üí° Run: python pipeline/mlb_pipeline.py --refresh-players")
                else:
                    logger.warning("üìÇ No games found in database")
                
                # Check for pitcher data
                pitcher_query = "SELECT COUNT(*) FROM game_starting_pitchers"
                try:
                    cursor = conn.cursor()
                    cursor.execute(pitcher_query)
                    pitcher_count = cursor.fetchone()[0]
                    logger.info(f"‚öæ Starting pitcher records: {pitcher_count:,}")
                except:
                    logger.warning("‚ö†Ô∏è No game_starting_pitchers table found")
                
                # Check team statistics
                try:
                    team_stats_query = "SELECT COUNT(*) FROM team_statistics"
                    cursor.execute(team_stats_query)
                    team_stats_count = cursor.fetchone()[0]
                    logger.info(f"üèüÔ∏è Team statistics records: {team_stats_count:,}")
                except:
                    logger.warning("‚ö†Ô∏è No team_statistics table found")
                    
        except Exception as e:
            logger.error(f"‚ùå Database check failed: {e}")
    else:
        logger.warning(f"üìÇ Database not found: {model.db_path}")
        logger.info("üí° Run data ingestion to create database")


def main():
    """Main training function."""
    logger.info("üöÄ Starting MLB Model Training...")
    
    # Initialize and train model
    model = MLBPredictionModel()
    
    # Train all models
    results = model.train_all_models()
    
    if results:
        # Save the best model
        model.save_model()
        logger.info("üéØ Training completed successfully!")
        logger.info("üí° Test predictions with: python models/mlb/mlb_model.py --predict")
    else:
        logger.error("‚ùå Training failed - no models trained successfully")


if __name__ == "__main__":
    import sys
    
    # Handle different command modes
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == '--test':
            logger.info("üß™ Running setup test...")
            success = test_model_setup()
            if success:
                logger.info("‚úÖ All tests passed! Ready for training.")
                logger.info("üöÄ Run without --test flag to start training")
            else:
                logger.error("‚ùå Tests failed - check issues above")
                
        elif command == '--predict':
            logger.info("üîÆ Testing predictions...")
            success = test_predictions()
            if success:
                logger.info("‚úÖ Prediction test successful!")
            else:
                logger.error("‚ùå Prediction test failed")
                
        elif command == '--check-data':
            check_data_availability()
            
        elif command == '--check-odds':
            check_odds_api_methods()
            
        elif command == '--debug-odds':
            debug_odds_api()
            
        elif command == '--help':
            print("üöÄ MLB Model Commands:")
            print("  python models/mlb/mlb_model.py              # Train model")
            print("  python models/mlb/mlb_model.py --test       # Test setup")
            print("  python models/mlb/mlb_model.py --predict    # Test predictions")
            print("  python models/mlb/mlb_model.py --check-data # Check available data")
            print("  python models/mlb/mlb_model.py --check-odds # Check Odds API methods")
            print("  python models/mlb/mlb_model.py --debug-odds # Debug odds API issues")
            
        else:
            logger.error(f"‚ùå Unknown command: {command}")
            logger.info("üí° Use --help for available commands")
    else:
        main()
